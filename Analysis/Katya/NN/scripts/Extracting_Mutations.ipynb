{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katya/anaconda3/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run './Functions.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming weights into a heatmap-like df\n",
    "\n",
    "def make_tables(chunk, n):\n",
    "    index = set([x[-1] for x in unique_mutations[chunk]])\n",
    "    columns = set([int(x[:-1]) for x in unique_mutations[chunk]])\n",
    "    df_0 = pd.DataFrame(index=sorted(index), columns=columns).astype(float) # dummy dfs for each of the entrance neurons\n",
    "    df_1 = pd.DataFrame(index=sorted(index), columns=columns).astype(float)\n",
    "    \n",
    "    for i in range(len(train_weights[chunk][n])):\n",
    "        position = unique_mutations[chunk][i][:-1]\n",
    "        mutation = unique_mutations[chunk][i][-1]\n",
    "        df_0[int(position)][mutation] = (train_weights[chunk][n][i][0]).astype(float)\n",
    "        df_1[int(position)][mutation] = (train_weights[chunk][n][i][1]).astype(float)\n",
    "        \n",
    "    return df_0,df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_differences(chunk, n):\n",
    "    mask = {}\n",
    "    df = {}\n",
    "    \n",
    "    mask[0],mask[1] = make_tables(chunk,n)\n",
    "    for key in mask:\n",
    "        mask[key][mask[key]>0]=1\n",
    "        mask[key][mask[key]<0]=-1\n",
    "    \n",
    "    mask = (mask[0] - mask[1]).fillna(0)\n",
    "    df[0],df[1] = make_tables(chunk,n)\n",
    "    \n",
    "    return (df[0][mask!=0]-df[1][mask!=0]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_occurences(chunk,n):\n",
    "    effects = {}\n",
    "    for n in range(n):\n",
    "        result = extract_differences(chunk, n)\n",
    "        for i in result.index:\n",
    "            for j in result.columns:\n",
    "                if result[j].ix[i]!=0:\n",
    "                    if str(j)+str(i) not in effects.keys():\n",
    "                        effects[str(j)+str(i)] = []\n",
    "                        effects[str(j)+str(i)].append(result[j].ix[i])\n",
    "                    else:\n",
    "                        effects[str(j)+str(i)].append(result[j].ix[i])\n",
    "    return effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training models and extracting weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Test conditions on one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katya/anaconda3/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:552: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=20.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "chunk = 'S5'\n",
    "\n",
    "data, labels = read_data(chunk)\n",
    "\n",
    "train_weights[chunk]={}\n",
    "cv = cross_validation.StratifiedKFold(labels, n_folds = 20, shuffle = True)\n",
    "counter = 0\n",
    "\n",
    "for train_idx, test_idx in cv:\n",
    "    print (counter)\n",
    "    X_train, y_train = data[train_idx,:], labels[train_idx]\n",
    "    X_test, y_test = data[test_idx,:], labels[test_idx]\n",
    "\n",
    "    model = Sequential()\n",
    "    init = initializers.Orthogonal(gain=1.0, seed=None)\n",
    "\n",
    "    model.add(Dense(2, input_dim=data.shape[1], activation='sigmoid', kernel_initializer=init))\n",
    "    model.add(Dense(100, activation='sigmoid', kernel_initializer=init))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=init))\n",
    "\n",
    "    opt = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='mean_squared_error',\n",
    "                  verbose=0)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=500, batch_size=500, shuffle=True, verbose=0)\n",
    "    train_weights[chunk][counter] = model.layers[0].get_weights()[0]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S5\n",
      "S5 0 0.240440462253\n",
      "S5 1 0.250738901426\n",
      "S5 2 0.2650773999\n",
      "S5 3 0.277421680114\n",
      "S5 4 -0.0740884483528\n",
      "S5 5 0.276924692172\n",
      "S5 6 0.391093456367\n",
      "S5 7 -0.395858567387\n",
      "S5 8 -0.134742180854\n",
      "S5 9 0.211869312216\n",
      "S5 10 0.207364638688\n",
      "S5 11 0.3699454901\n",
      "S5 12 0.258245815706\n",
      "S5 13 -0.439798662858\n",
      "S5 14 -0.130604731684\n",
      "S5 15 0.0508628012192\n",
      "S5 16 -0.499467209335\n",
      "S5 17 0.480540452372\n",
      "S5 18 -0.489251086085\n",
      "S5 19 0.372092915496\n"
     ]
    }
   ],
   "source": [
    "for chunk in ['S5']:\n",
    "    print chunk\n",
    "    for i in range(20):\n",
    "        _,_,R,_,_ = stats.linregress(train_weights[chunk][i][:,0],train_weights[chunk][i][:,1])\n",
    "\n",
    "        print chunk,i,R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S5\n",
      "2T\t0.543092928827\t0.165919797132\t11\n",
      "25L\t1.61543478072\t0.448285557388\t14\n",
      "21A\t2.54797014594\t1.98931401822\t14\n",
      "25S\t0.929264426231\t0.253345615892\t12\n",
      "22S\t2.54611752182\t0.68615643076\t11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toprint = {}\n",
    "for chunk in ['S5']:\n",
    "    print chunk\n",
    "    d = count_occurences(chunk,20)\n",
    "    for key in d:\n",
    "        if len(d[key])>10:\n",
    "            print key+'\\t'+str(np.median([np.abs(x) for x in d[key]]))+'\\t'+str(np.std([np.abs(x) for x in d[key]]))+'\\t'+str(len(d[key]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Performing the operation on all the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_weights = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "    print chunk\n",
    "    data, labels = read_data(chunk)\n",
    "    train_weights[chunk]={}\n",
    "    cv = cross_validation.StratifiedKFold(labels, n_folds = 10, shuffle = True)\n",
    "    counter = 0\n",
    "\n",
    "    for train_idx, test_idx in cv:\n",
    "        print (counter)\n",
    "        X_train, y_train = data[train_idx,:], labels[train_idx]\n",
    "        X_test, y_test = data[test_idx,:], labels[test_idx]\n",
    "\n",
    "        model = Sequential()\n",
    "        init = initializers.Orthogonal(gain=1.0, seed=None)\n",
    "\n",
    "        model.add(Dense(2, input_dim=data.shape[1], activation='sigmoid', kernel_initializer=init))\n",
    "        model.add(Dense(100, activation='sigmoid', kernel_initializer=init))\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_initializer=init))\n",
    "\n",
    "        opt = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss='mean_squared_error',\n",
    "                      verbose=0)\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=500, batch_size=500, shuffle=True, verbose=0)\n",
    "        train_weights[chunk][counter] = model.layers[0].get_weights()[0]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toprint = {}\n",
    "for chunk in chunks:\n",
    "    print chunk\n",
    "    d = count_occurences(chunk,10)\n",
    "    for key in d:\n",
    "        if len(d[key])>5:\n",
    "            print key+'\\t'+str(np.median([np.abs(x) for x in d[key]]))+'\\t'+str(np.std([np.abs(x) for x in d[key]]))+'\\t'+str(len(d[key]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toprint = {}\n",
    "for chunk in chunks:\n",
    "    print chunk\n",
    "    d = count_occurences(chunk,10)\n",
    "    for key in d:\n",
    "        if len(d[key])>5:\n",
    "            print key+'\\t'+str(np.median([np.abs(x) for x in d[key]]))+'\\t'+str(np.std([np.abs(x) for x in d[key]]))+'\\t'+str(len(d[key]))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print chunk\n",
    "    for i in range(10):\n",
    "        _,_,R,_,_ = stats.linregress(train_weights[chunk][i][:,0],train_weights[chunk][i][:,1])\n",
    "        if R < 0:\n",
    "            print chunk,i,R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
