{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S9\n",
      "Splitting the data\n",
      "\n",
      "Number of weights combinations =  1\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2555 - val_loss: 0.2298\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2290 - val_loss: 0.2361\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2275 - val_loss: 0.2286\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2229\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2220\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2234\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2215\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2258\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2215\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2299\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2248\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2199\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2237\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2245\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2282\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2365\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2224\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2618\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2264\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2238\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2220\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2397\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2320\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2317 - val_loss: 0.2475\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2285 - val_loss: 0.2424\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2377\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2572\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2212\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2346\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2250\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2280\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2472\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2214\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2264\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2265\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2279\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2585\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2313\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2209\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2302\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2390\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2303\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2242\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2240\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2284\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2207\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2206\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2259\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2240\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2499\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2209\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2204\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2256\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2436\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2257\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2220\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2324\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2211\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2200\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2230\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2442\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2340\n",
      "Epoch 40/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2203\n",
      "Epoch 41/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2458\n",
      "Epoch 42/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2241\n",
      "Epoch 43/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2203\n",
      "Epoch 44/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2272\n",
      "Epoch 45/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2204\n",
      "Epoch 46/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2203\n",
      "Epoch 47/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2331\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2500 - val_loss: 0.2394\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2266 - val_loss: 0.2280\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2277 - val_loss: 0.2258\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2285\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2244\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2233\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2246\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2221\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2215\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2263\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2480\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2209\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2217\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2258\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2478\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2231\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2206\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2243\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2276\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2210\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2213\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2208\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2211\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2215\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2239\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2318\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2208\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2271\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2306 - val_loss: 0.2347\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2289 - val_loss: 0.2274\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2325\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2271\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2212\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2364\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2216\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2206\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2234\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2226\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2343\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2225\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2207\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2362\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2248\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2208\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2208\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2236\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2206\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2234\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2207\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2469\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2206\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2205\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2310\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2205\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2286\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2233\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2221\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2208\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2206\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2203\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2264\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2249\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2205\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2515\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2204\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2216\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2201\n",
      "Epoch 40/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2349\n",
      "Epoch 41/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2253\n",
      "Epoch 42/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2213\n",
      "Epoch 43/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2436\n",
      "Epoch 44/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2255\n",
      "Epoch 45/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2210\n",
      "Epoch 46/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2231\n",
      "Epoch 47/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2469\n",
      "Epoch 48/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2303\n",
      "Epoch 49/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2328\n",
      "Epoch 50/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2273\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2344 - val_loss: 0.2356\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2275 - val_loss: 0.2290\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2256 - val_loss: 0.2268\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2263 - val_loss: 0.2236\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2229\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2269\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2231\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2245\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2270\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2233\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2284\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2213\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2222\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2217\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2316\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2205\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2370\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2275\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2279\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2275\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2230\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2227\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2276\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2454\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2385\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2242\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2232\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2396 - val_loss: 0.2390\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2290 - val_loss: 0.2331\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2281\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2563\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2225\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2233\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2654\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2243\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2247\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2303\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2209\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2263\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2479\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2206\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2269\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2660\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2357\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2215\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2209\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2358\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2624\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2293\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2342\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2365\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2249\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2315 - val_loss: 0.2298\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2274 - val_loss: 0.2276\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2256\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2256 - val_loss: 0.2247\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2377\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2237\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2363\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2260\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2302\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2348\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2233\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2283\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2226\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2216\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2224\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2210\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2223\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2213\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2214\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2221\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2210\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2321\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2463\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2207\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2352\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2261\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2260\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2261\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2301\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2209\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2403\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2221\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2264\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2202\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2300\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2280\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2293\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2343\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2284\n",
      "Epoch 40/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2336\n",
      "Epoch 41/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2215\n",
      "Epoch 42/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2221\n",
      "Epoch 43/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2323\n",
      "Epoch 44/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2209\n",
      "Epoch 45/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2203\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2357 - val_loss: 0.2292\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2283 - val_loss: 0.2305\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2283 - val_loss: 0.2248\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2291\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2506\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2218\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2245\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2341\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2257\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2219\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2274\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2265\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2205\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2224\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2222\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2213\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2273\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2293\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2235\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2208\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2225\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2258\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2210\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2219\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2305 - val_loss: 0.2285\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2263 - val_loss: 0.2429\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2262 - val_loss: 0.2217\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2203\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2205\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2579\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2236\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2214\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2215\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2205\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2269\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2233\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2434\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2337\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2229\n",
      "19\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2316 - val_loss: 0.2288\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2392\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2303\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2246\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2380\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2245\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2273\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2203\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2488\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2226\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2207\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2228\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2213\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2214\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2251\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2277\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2213\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2232\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2256\n",
      "\n",
      "Number of weights combinations =  5\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2560 - val_loss: 0.2285\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2286 - val_loss: 0.2275\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2257\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2225\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2258\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2223\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2213\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2276\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2371\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2529\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2307\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2203\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2201\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2203\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2234\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2350\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2234\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2305\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2288\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2368\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2207\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2344\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2348\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2201\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2339 - val_loss: 0.2349\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2276 - val_loss: 0.2448\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2372\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2300\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2271\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2238\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2471\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2216\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2212\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2212\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2216\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2216\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2211\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2230\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2232\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2486\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2432\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2227\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2294\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2234\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2450\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2227\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2231\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2273\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2314 - val_loss: 0.2408\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2292 - val_loss: 0.2270\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2334\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2224\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2256 - val_loss: 0.2253\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2507\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2207\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2329\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2255\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2301\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2196\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2224\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2269\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2364\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2197\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2272\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2243\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2245\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2375\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2249\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2228\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2250\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3400 - val_loss: 0.2279\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2349\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2502\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2223\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2283\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2377\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2211\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2229\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2210\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2209\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2230\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2258\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2248\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2242\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2222\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2216\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2495\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2213\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2288\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2255\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2231\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2527 - val_loss: 0.2301\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2282 - val_loss: 0.2262\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2436\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2276 - val_loss: 0.2238\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2219\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2508\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2225\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2408\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2214\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2233\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2349\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2238\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2243\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2262\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2245\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2506\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2222\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2227\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2231\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2222\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3353 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3358 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "19\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "20\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2350 - val_loss: 0.2638\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2285 - val_loss: 0.2283\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2288\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2218\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2379\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2390\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2235\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2325\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2252\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2208\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2373\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2279\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2258\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2416\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2273\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2259\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2212\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2210\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2322\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2244\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2224\n",
      "21\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3360 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "22\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2325 - val_loss: 0.2257\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2240\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2229\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2213\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2213\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2208\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2237\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2237\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2246\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2209\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2231\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2272\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2211\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2229\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2418\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2288\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2209\n",
      "23\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "24\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3361 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "25\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "26\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "27\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2333 - val_loss: 0.2277\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2285 - val_loss: 0.2290\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2276 - val_loss: 0.2271\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2233\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2218\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2404\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2214\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2464\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2295\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2308\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2208\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2214\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2248\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2215\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2193\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2322ss: 0.21\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2220\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2228\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2308\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2185\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2259\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2185\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2190\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2200\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2255\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2250\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2190\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2182\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2198\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2235\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2223\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2256\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2358\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2200\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2349\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2246\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2202\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2195\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2196\n",
      "28\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2338 - val_loss: 0.2297\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2260\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2246\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2248\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2245\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2258 - val_loss: 0.2309\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2229\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2216\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2218\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2211\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2219\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2331\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2217\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2202\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2201\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2213\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2274\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2331\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2259\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2448\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2284\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2249\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2280\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2219\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2214\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2278\n",
      "29\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "30\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2327 - val_loss: 0.2441\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2289\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2262 - val_loss: 0.2248\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2226\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2226\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2222\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2212\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2224\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2214\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2275\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2397\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2223\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2297\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2204\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2238\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2325\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2224\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2252\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2237\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2295\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2230\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2240\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2315\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2320\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2223\n",
      "\n",
      "Number of weights combinations =  9\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2437 - val_loss: 0.2277\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2298 - val_loss: 0.2364\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2239\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2282 - val_loss: 0.2266\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2361\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2405\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2249\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2244\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2331\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2215\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2204\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2223\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2470\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2251\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2234\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2357\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2273\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2243\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2212\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2201\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2198\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2574\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2220\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2215\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2307\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2226\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2208\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2390\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2312\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2405\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2311\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2245\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2517 - val_loss: 0.2264\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2294 - val_loss: 0.2329\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2281 - val_loss: 0.2212ss: 0.23\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2226\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2342\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2352\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2204\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2204\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2207\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2204\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2372\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2208\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2593\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2204\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2201\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2226\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2217\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2233\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2295\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2246\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2396\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2274\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2209\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2218\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2326\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2310\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2368 - val_loss: 0.2396\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2294 - val_loss: 0.2282\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2274 - val_loss: 0.2397\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2750\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2253\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2576\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2262 - val_loss: 0.2233\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2245\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2236\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2221\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2234\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2323\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2324\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2232\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2414\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2257\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2348\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2300\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2280\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2214\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2237\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2360ss: 0.22\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2379\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2624\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2413\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2239\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2328\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2404\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2648\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2290\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2240\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2366 - val_loss: 0.2278\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2298 - val_loss: 0.2237\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2293\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2321\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2275\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2254\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2320\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2261\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2221\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2199\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2471\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2196\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2201\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2204\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2446\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2205\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2206ss: 0.22\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2246\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2272\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2231\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2192\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2235\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2206ss: 0.21\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2208ss: 0.21\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2194\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2338\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2280\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2202\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2310\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2337\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2298\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2196\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.34\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2330 - val_loss: 0.2282\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2240\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2426\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2593\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2298\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2505\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2853\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2216\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2221\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2282\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2245\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2257\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2270\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2217\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2212\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2352\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2208\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2280\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2250\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2305\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2255\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2236\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2266\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2335\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2298\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2215\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2208\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2216\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2254\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2282\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2205\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2218\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2186 - val_loss: 0.2280\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2237\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2303\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2213\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2186 - val_loss: 0.2201\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2223\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2173 - val_loss: 0.2211\n",
      "Epoch 40/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2292\n",
      "Epoch 41/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2195\n",
      "Epoch 42/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2220\n",
      "Epoch 43/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2178 - val_loss: 0.2205\n",
      "Epoch 44/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2179 - val_loss: 0.2224\n",
      "Epoch 45/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2178 - val_loss: 0.2190\n",
      "Epoch 46/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2178 - val_loss: 0.2217\n",
      "Epoch 47/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2178 - val_loss: 0.2199\n",
      "Epoch 48/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2176 - val_loss: 0.2238\n",
      "Epoch 49/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2175 - val_loss: 0.2193\n",
      "Epoch 50/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2173 - val_loss: 0.2237\n",
      "Epoch 51/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2170 - val_loss: 0.2195\n",
      "Epoch 52/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2174 - val_loss: 0.2198\n",
      "Epoch 53/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2170 - val_loss: 0.2244\n",
      "Epoch 54/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2177 - val_loss: 0.2244\n",
      "Epoch 55/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2167 - val_loss: 0.2209\n",
      "Epoch 56/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2174 - val_loss: 0.2246\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3374 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.33\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2393 - val_loss: 0.2331\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2273 - val_loss: 0.2263\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2263 - val_loss: 0.2225\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2334\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2355\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2370\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2211ss: 0.22\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2241\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2243\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2315\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2258\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2603\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2264\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2198\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2201\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2211\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2216\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2253\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2535\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2209\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2197\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2331\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2238\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2262\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2213\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2236\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2313\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2447\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2204\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2307\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2208\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2221\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.34\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2555 - val_loss: 0.2285\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2305 - val_loss: 0.2255\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2281 - val_loss: 0.2300\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2439\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2215\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2230\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2238\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2222\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2250\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2461\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2262\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2493\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2281\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2206\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2208\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2213\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2201\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2210\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2203\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2214\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2573\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2212\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2199\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2208\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2284\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2212\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2194\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2192\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2194\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2269\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2215\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2193\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2320\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2210\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2195\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2260\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2194\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2201\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2225\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.33\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2436 - val_loss: 0.2309\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2322 - val_loss: 0.2241\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2288 - val_loss: 0.2256\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2287 - val_loss: 0.2345\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2228\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2233\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2240\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2218\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2241\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2269\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2212\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2285\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2246\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2252\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2343\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2242\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2222\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2200\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2414ss: 0.21\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2217\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2246\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2548\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2212\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2295\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2242\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2219\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2218\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2253\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2231\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2381 - val_loss: 0.2369\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2415\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2298\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2406\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2223\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2269\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2278\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2256ss: 0.22\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2237\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2265\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2384\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2235\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2220\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2218\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2246\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2221\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2237\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2267\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2242\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2227\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2218\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2420\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2232\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2245\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2242\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2241ss: 0.21\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2353\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2223\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2188 - val_loss: 0.2245\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2220\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2236\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2255\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2348 - val_loss: 0.2497\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2271 - val_loss: 0.2224\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2316\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2565\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2232\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2251\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2320\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2250\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2317\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2207\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2203\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2223\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2255\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2204\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2279\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2260\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2211\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2203\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2265\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2322\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2207\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2262\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2283\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2222\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2211\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2213\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2263\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2234\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2177 - val_loss: 0.2235\n",
      "\n",
      "Number of weights combinations =  13\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.33\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3358 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2392 - val_loss: 0.2242\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2291 - val_loss: 0.2363\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2257 - val_loss: 0.2242\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2464\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2211\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2224\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2309\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2268\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2214\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2343\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2230\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2214\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2421\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2197\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2207\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2201\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2201\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2213\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2213\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2245\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2198\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2246\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2254\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2200\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2267\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3362 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.33\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2398 - val_loss: 0.2451\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2303 - val_loss: 0.2312\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2261\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2225\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2241\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2218\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2298\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2412\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2236\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2345\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2257\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2410\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2243\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2217\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2411\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2204\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2243\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2259\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2206\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2245\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2205\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2290\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2224\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2227\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2249\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2226\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2289\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.34\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2411 - val_loss: 0.2283\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2323 - val_loss: 0.2409\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2237\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2437\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2634\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2286\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2224\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2223\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2327\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2208\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2207\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2217\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2233\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2212\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2368\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2405\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2215\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2257\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2277\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2301\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2644\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2458\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2780 - val_loss: 0.2507\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2326 - val_loss: 0.2329\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2288 - val_loss: 0.2556\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2275 - val_loss: 0.2230ss: 0.228\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2283 - val_loss: 0.2297\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2250\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2248\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2441\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2238\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2234\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2245\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2238\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2208\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2209\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2225\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2252\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2310\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2252\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2238\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2189\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2184\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2182\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2186\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2188\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2188 - val_loss: 0.2229\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2396\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2288\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2211\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2180 - val_loss: 0.2191\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2195\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2176 - val_loss: 0.2207\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2180 - val_loss: 0.2194\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2176 - val_loss: 0.2209\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2641 - val_loss: 0.2378\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2324 - val_loss: 0.2468\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2300 - val_loss: 0.2225\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2481\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2212\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2234\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2277\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2694\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2257 - val_loss: 0.2317\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2261\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2216\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2288\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2254\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2276\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2214\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2235\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2432 - val_loss: 0.2459\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2310 - val_loss: 0.2256\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2291 - val_loss: 0.2293\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2275 - val_loss: 0.2300\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2248\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2211\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2242\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2211\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2241 - val_loss: 0.2670\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2201\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2267\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2308\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2348\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2265\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2253\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2197\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2417\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2207\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2277\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2268\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2237\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2270\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2219\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2242\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2225\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2249\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2179 - val_loss: 0.2289\n",
      "19\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2550 - val_loss: 0.2273\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2318 - val_loss: 0.2246\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2644\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2381\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2262 - val_loss: 0.2244\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2225\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2361\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2237\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2432\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2234\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2410\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2283\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2227\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2227\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2298\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2203\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2233\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2197\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2208\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2260\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2215\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2210\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2241\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2219\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2208\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2188 - val_loss: 0.2500\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2211\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2252\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2188 - val_loss: 0.2243\n",
      "20\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2446 - val_loss: 0.2277\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2312 - val_loss: 0.2271\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2636\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2227\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2320\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2215\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2311\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2242\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2251\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2259\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2213\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2360\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2288\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2213\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2211\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2220\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2208\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2260\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2243\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2245\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2212\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2213\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2219\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2245\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2188 - val_loss: 0.2521\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2397\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2235\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2278\n",
      "21\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "22\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2438 - val_loss: 0.2453\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2314 - val_loss: 0.2594\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2286 - val_loss: 0.2409\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2302\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2229\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2273 - val_loss: 0.2327\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2303\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2291ss: 0.226\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2479\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2319\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2430\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2272\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2331\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2285\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2240\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2246\n",
      "23\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "24\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3374 - val_loss: 0.3680\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "25\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "26\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2409 - val_loss: 0.2708\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2315 - val_loss: 0.2299\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2287 - val_loss: 0.2222\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2215\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2224\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2429\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2294\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2352\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2211\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2262\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2227\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2202\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2216\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2330\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2210\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2397\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2203\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2211\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2572\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2205\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2297\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2230\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2216\n",
      "\n",
      "Number of weights combinations =  17\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3366 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680ss: 0.336\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2391 - val_loss: 0.2300\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2293 - val_loss: 0.2282\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2758\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2257 - val_loss: 0.2227\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2369\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2312\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2276\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2210\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2195\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2286\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2224\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2199\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2558\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2453\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2238\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2216\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2219\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2231\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2451\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2263\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2403 - val_loss: 0.2277\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2300 - val_loss: 0.2378\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2295\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2631\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2216\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2218\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2204\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2595\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2465\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2204\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2354\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2205\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2300\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2481\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2200\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2210\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2200\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2199\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2234\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2312\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2257\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2338\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2253\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2282\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2237\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2343\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2300\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2314\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2400\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2557 - val_loss: 0.2341\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2312 - val_loss: 0.2233\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2298 - val_loss: 0.2423\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2258 - val_loss: 0.2258\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2217\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2393\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2388\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2213\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2361\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2339\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2216\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2429\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2220\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2280\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2249\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2260\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2261\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2330\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2306\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2619 - val_loss: 0.2278\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2243\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2633\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2352\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2273\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2270\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2251\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2292\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2346\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2234\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2621\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2469ss: 0.223\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2246\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2257\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2235\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2208\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2310\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2236\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2207\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2203\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2226\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2234\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2199ss: 0.218\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2309\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2217\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2242\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2213ss: 0.218\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2419\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2270\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2226\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2213\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2186 - val_loss: 0.2359\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2186 - val_loss: 0.2241\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2227\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2589 - val_loss: 0.2283\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2307 - val_loss: 0.2542\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2257\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2259 - val_loss: 0.2407\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2224\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2239\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2234\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2365\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2223\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2325\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2230\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2478\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2262\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2241\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2525\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2229\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2534\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2254\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2212\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2213 - val_loss: 0.2301\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2377\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2232\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2223\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2285\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2289\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2228\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2236\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2236\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2203 - val_loss: 0.2344\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2384\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3356 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3356 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2427 - val_loss: 0.2501\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2295 - val_loss: 0.2528\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2287 - val_loss: 0.2269\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2379\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2243\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2219\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2367\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2377\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2300\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2220\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2259\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2204\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2214\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2321\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2297\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2220\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2236\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2203\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2210\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2211\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2198\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2204\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2227\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2321\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2313\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2202\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2218\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2186 - val_loss: 0.2227\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2264\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2363\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2174 - val_loss: 0.2375\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2180 - val_loss: 0.2238\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2483 - val_loss: 0.2903\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2352 - val_loss: 0.2315\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2304 - val_loss: 0.2234\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2234\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2277 - val_loss: 0.2219\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2215\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2204\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2194\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2726\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2210\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2253\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2194\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2198\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2231\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2197\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2204\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2240\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2213\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2198\n",
      "19\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "20\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2407 - val_loss: 0.2270\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2303 - val_loss: 0.2499\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2274 - val_loss: 0.2237\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2270\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2425\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2215\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2230\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2213\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2293\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2228\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2344\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2206\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2235\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2261\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2308\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2274\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2439\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2237\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2206\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2380\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2255\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2192 - val_loss: 0.2209\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2248\n",
      "21\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3362 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "22\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "23\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2554 - val_loss: 0.2304\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2319 - val_loss: 0.2605\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2322 - val_loss: 0.2235\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2296 - val_loss: 0.2223\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2323 - val_loss: 0.2234\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2249\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2263 - val_loss: 0.2225\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2276 - val_loss: 0.2264\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2482\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2407\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2235\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2268 - val_loss: 0.2391\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2322\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2235\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2300\n",
      "24\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2480 - val_loss: 0.2352\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2304 - val_loss: 0.2349\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2312 - val_loss: 0.2289\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2277 - val_loss: 0.2235\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2258 - val_loss: 0.2409\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2220\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2221\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2277\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2314\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2205\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2206\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2218\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2238\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2225\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2271\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2338\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2221\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2225\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2245\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2215\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2206\n",
      "\n",
      "Number of weights combinations =  21\n",
      "0\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "1\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2604 - val_loss: 0.3360\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2378 - val_loss: 0.2264\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2315 - val_loss: 0.2534\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2278 - val_loss: 0.2324\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2283 - val_loss: 0.2510\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2264\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2339\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2253 - val_loss: 0.2222\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2247\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2270\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2216\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2275\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2220\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2299\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2269\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2395\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2277\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2429\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2302\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2226\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2252ss: 0.222\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2262\n",
      "2\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "3\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "4\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2458 - val_loss: 0.2442\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2290 - val_loss: 0.2263\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2224\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2272 - val_loss: 0.2217\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2469\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2225\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2411\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2280\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2226\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2221\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2227\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2215\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2211\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2216\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2211\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2215\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2233\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2219\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2216\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2201 - val_loss: 0.2217\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2385\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2224\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2219\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2235\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2281\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2230\n",
      "5\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.2590 - val_loss: 0.2563\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2327 - val_loss: 0.2265\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2295 - val_loss: 0.2258\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2308 - val_loss: 0.2225\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2265 - val_loss: 0.2270\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2251 - val_loss: 0.2412\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2352\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2263 - val_loss: 0.2241\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2213\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2349\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2240\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2207\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2403\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2457\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2205\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2220\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2264\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2379\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2224\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2215\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2428\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2212\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2210\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2198\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2284\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2233\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2213\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2201\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2253\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2257\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2242\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2243\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2218\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2258\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2205\n",
      "6\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 3s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "7\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2595 - val_loss: 0.2392\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2313 - val_loss: 0.2225\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2293 - val_loss: 0.2262\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2291 - val_loss: 0.2446\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2267 - val_loss: 0.2226\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2288\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2277\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2287\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2258\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2246 - val_loss: 0.2210\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2224\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2325\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2221\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2227\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2240 - val_loss: 0.2224\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2222\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2219 - val_loss: 0.2501\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2230\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2221\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2261\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2221\n",
      "8\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "9\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "10\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "11\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3374 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "12\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2599 - val_loss: 0.2397\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2381 - val_loss: 0.2244\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2303 - val_loss: 0.2371\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2266 - val_loss: 0.2310\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2267 - val_loss: 0.2501\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2252 - val_loss: 0.2615\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2248 - val_loss: 0.2214\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2253\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2232 - val_loss: 0.2215\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2341\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2227 - val_loss: 0.2226\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2233 - val_loss: 0.2219\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2229 - val_loss: 0.2210\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2492\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2263\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2771\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2507\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2405\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2216 - val_loss: 0.2572\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2283\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2258\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2236\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2277\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2212\n",
      "13\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2475 - val_loss: 0.2264\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2287 - val_loss: 0.2277\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2310 - val_loss: 0.2241\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2403\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2254 - val_loss: 0.2318\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2305\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2435\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2247 - val_loss: 0.2407\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2208\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2247\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2202\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2224 - val_loss: 0.2203\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2223\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2252\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2232\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2529\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2327\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2204\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2212 - val_loss: 0.2404\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2239\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2252\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2211\n",
      "14\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 4s - loss: 0.2525 - val_loss: 0.2949\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2355 - val_loss: 0.2267\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2293 - val_loss: 0.2466\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2231\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2261 - val_loss: 0.2244\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2242 - val_loss: 0.2276\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2407\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2285\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2244 - val_loss: 0.2219\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2211\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2234 - val_loss: 0.2215\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2507\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2235 - val_loss: 0.2295\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2436\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2303\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2249\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2200\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2211 - val_loss: 0.2306\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2223 - val_loss: 0.2215\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2241\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2250\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2209 - val_loss: 0.2317\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2210 - val_loss: 0.2405\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2205 - val_loss: 0.2214\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2221\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2268\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2249\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2257\n",
      "15\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "16\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3359 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "17\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "18\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "19\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2517 - val_loss: 0.2316\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.2326 - val_loss: 0.2263\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2310 - val_loss: 0.2293\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2280 - val_loss: 0.2249\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2319\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2264 - val_loss: 0.2365\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2484\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2362\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2245 - val_loss: 0.2219\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2243 - val_loss: 0.2274\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2267\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2228 - val_loss: 0.2221\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2333\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2206 - val_loss: 0.2236\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2226 - val_loss: 0.2286\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2318\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2303\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2408\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2196 - val_loss: 0.2345\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2206\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2200 - val_loss: 0.2218\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2288\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2193 - val_loss: 0.2280\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2281\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2189 - val_loss: 0.2245\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2223\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2217\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2236\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2171 - val_loss: 0.2398\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2205\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2175 - val_loss: 0.2255\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2215\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2167 - val_loss: 0.2262\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2171 - val_loss: 0.2226\n",
      "Epoch 35/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2166 - val_loss: 0.2304\n",
      "Epoch 36/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2168 - val_loss: 0.2209\n",
      "Epoch 37/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2165 - val_loss: 0.2216\n",
      "Epoch 38/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2166 - val_loss: 0.2218\n",
      "Epoch 39/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2161 - val_loss: 0.2229\n",
      "Epoch 40/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2158 - val_loss: 0.2355\n",
      "Epoch 41/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2157 - val_loss: 0.2222\n",
      "20\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "21\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "22\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "23\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.3458 - val_loss: 0.3680\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.3390 - val_loss: 0.3680\n",
      "24\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2468 - val_loss: 0.2578\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2352 - val_loss: 0.2272\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2302 - val_loss: 0.2270\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2266 - val_loss: 0.2232\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2262 - val_loss: 0.2226\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2260 - val_loss: 0.2245\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2250 - val_loss: 0.2524\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2249 - val_loss: 0.2208\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2226\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2513\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2239 - val_loss: 0.2228\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2199\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2214 - val_loss: 0.2479\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2437\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2314\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2222 - val_loss: 0.2299\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2355\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2217 - val_loss: 0.2378\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2207 - val_loss: 0.2252\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2222\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2218 - val_loss: 0.2307\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2250\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2193\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2202 - val_loss: 0.2207\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2325\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2410\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2272\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2225\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2233\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2238\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2291\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2184 - val_loss: 0.2239\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2326\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2180 - val_loss: 0.2267\n",
      "25\n",
      "Train on 16682 samples, validate on 169 samples\n",
      "Epoch 1/100\n",
      "16682/16682 [==============================] - 4s - loss: 0.2444 - val_loss: 0.2282\n",
      "Epoch 2/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2321 - val_loss: 0.2248\n",
      "Epoch 3/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2255 - val_loss: 0.2228\n",
      "Epoch 4/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2270 - val_loss: 0.2335\n",
      "Epoch 5/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2238 - val_loss: 0.2519\n",
      "Epoch 6/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2231 - val_loss: 0.2573\n",
      "Epoch 7/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2237 - val_loss: 0.2370\n",
      "Epoch 8/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2236 - val_loss: 0.2205\n",
      "Epoch 9/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2244\n",
      "Epoch 10/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2225 - val_loss: 0.2210\n",
      "Epoch 11/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2221 - val_loss: 0.2288\n",
      "Epoch 12/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2229\n",
      "Epoch 13/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2220 - val_loss: 0.2220\n",
      "Epoch 14/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2312\n",
      "Epoch 15/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2215 - val_loss: 0.2312\n",
      "Epoch 16/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2237\n",
      "Epoch 17/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2195\n",
      "Epoch 18/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2197 - val_loss: 0.2209\n",
      "Epoch 19/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2204 - val_loss: 0.2194\n",
      "Epoch 20/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2199 - val_loss: 0.2367\n",
      "Epoch 21/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2190 - val_loss: 0.2204\n",
      "Epoch 22/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2198 - val_loss: 0.2335\n",
      "Epoch 23/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2187 - val_loss: 0.2190\n",
      "Epoch 24/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2194 - val_loss: 0.2190\n",
      "Epoch 25/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2191 - val_loss: 0.2221\n",
      "Epoch 26/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2204\n",
      "Epoch 27/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2232\n",
      "Epoch 28/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2182 - val_loss: 0.2252\n",
      "Epoch 29/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2264\n",
      "Epoch 30/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2178 - val_loss: 0.2203\n",
      "Epoch 31/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2185 - val_loss: 0.2242\n",
      "Epoch 32/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2176 - val_loss: 0.2363\n",
      "Epoch 33/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2173 - val_loss: 0.2339\n",
      "Epoch 34/100\n",
      "16682/16682 [==============================] - 0s - loss: 0.2171 - val_loss: 0.2281\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from optparse import OptionParser\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#min_max_scaler = MinMaxScaler()\n",
    "n_iter = 100\n",
    "\n",
    "chunk = 'S9'\n",
    "print (chunk)\n",
    "\n",
    "data, labels, unique_mutations[chunk], aa_seq, mut_list = read_data_all_positions(chunk)\n",
    "\n",
    "print 'Splitting the data'\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(data, labels, test_size = 0.01)\n",
    "\n",
    "n_neurons = []\n",
    "\n",
    "mse_val = []\n",
    "mse_train = []\n",
    "r_val=[]\n",
    "r_train=[]\n",
    "\n",
    "r2_weights = []\n",
    "\n",
    "for i in range(1,22,4):\n",
    "    \n",
    "    print '\\nNumber of weights combinations = ', i\n",
    "    temp_mse_train_list=[]\n",
    "    temp_mse_val_list=[]\n",
    "    temp_r_train_list=[]\n",
    "    temp_r_val_list=[]\n",
    "    temp_weights_r2={}\n",
    "    it=0\n",
    "    loop_count=10\n",
    "    \n",
    "    while it<loop_count and loop_count<100:\n",
    "        print it\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(i,input_dim=data.shape[1],activation='sigmoid',kernel_initializer='glorot_normal'))\n",
    "        model.add(Dense(20,activation='sigmoid'))\n",
    "        model.add(Dense(1,activation='relu'))\n",
    "\n",
    "        opt = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        \n",
    "        early_stopping_monitor=EarlyStopping(patience=10)\n",
    "        \n",
    "        model.compile(optimizer=opt,\n",
    "                      loss='mean_squared_error')\n",
    "\n",
    "        hist = model.fit(x_train, y_train, validation_data=[x_valid, y_valid], \n",
    "                                epochs=n_iter, batch_size=500, shuffle=True, callbacks=[early_stopping_monitor],verbose=1)\n",
    "\n",
    "        proba = model.predict_proba(x_valid, batch_size=500,verbose=0)\n",
    "        predicted_val = proba.flatten()\n",
    "\n",
    "        proba = model.predict_proba(x_train, batch_size=500,verbose=0)\n",
    "        predicted_train = proba.flatten()\n",
    "        \n",
    "        weights = model.layers[0].get_weights()[0]\n",
    "        \n",
    "        temp_mse_val = mean_squared_error(y_valid,predicted_val)\n",
    "        temp_mse_train = mean_squared_error(y_train,predicted_train)\n",
    "        temp_r_val = pearsonr(y_valid,predicted_val)[0]\n",
    "        temp_r_train = pearsonr(y_train,predicted_train)[0]\n",
    "        \n",
    "        it+=1\n",
    "        \n",
    "        #Sanity checks\n",
    "        if temp_mse_val<0.1:\n",
    "            temp_mse_val_list.append(temp_mse_val)\n",
    "            temp_mse_train_list.append(temp_mse_train)\n",
    "            temp_r_val_list.append(temp_r_val)\n",
    "            temp_r_train_list.append(temp_r_train)\n",
    "            \n",
    "            for combination in list(itertools.combinations([x for x in range(i)], 2)):\n",
    "                if combination in temp_weights_r2:\n",
    "                    temp_weights_r2[combination].extend([spearmanr(weights[:,combination[0]],weights[:,combination[1]])])\n",
    "                else:\n",
    "                    temp_weights_r2[combination] = [spearmanr(weights[:,combination[0]],weights[:,combination[1]])]\n",
    "                    \n",
    "        else:\n",
    "            loop_count+=1\n",
    "                    \n",
    "                    \n",
    "    n_neurons.append(i)\n",
    "    mse_val.append(temp_mse_val_list)\n",
    "    mse_train.append(temp_mse_train_list)\n",
    "    r_val.append(temp_r_val_list)\n",
    "    r_train.append(temp_r_train_list)\n",
    "    \n",
    "    if i>1:\n",
    "        r2_weights.append([np.median(temp_weights_r2[x]) for x in temp_weights_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAG+CAYAAABbMJnOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcG8mrKD+yfsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ac8a175a950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=[6,7])\n",
    "for i,r_list in enumerate(r_val):\n",
    "    plt.plot([i*4+1]*len(r_list),[float(x)**2 for x in r_list],'o',alpha=0.5,color='#283149')\n",
    "    plt.plot([i*4+0.5,i*4+1.5],[np.median([float(x)**2 for x in r_list])]*2,'-',lw=0.9,color='#DA0463')\n",
    "plt.grid('--k',lw=0.5)\n",
    "plt.title(chunk,fontsize=15)\n",
    "plt.ylabel('R2 between predicted and true values',fontsize=13)\n",
    "plt.xlabel('Number of linear combinations of substitutions',fontsize=13)\n",
    "plt.savefig('/nfs/scistore08/kondrgrp/eputints/Jupyter/HIS3InterspeciesEpistasis/Analysis/Katya/NN/complexity/20_iterations/r_'+chunk+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAG+CAYAAABbMJnOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ac8b98ea150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAG+CAYAAABfxUnzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CJJKs7gIkkqzuAiSSrO4CJJKu4b3aSeYTAw3KgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ac8b9a5cd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[6,7])\n",
    "for i,mse_list in enumerate(mse_val):\n",
    "    plt.plot([i*4+1]*len(mse_list),mse_list,'o',alpha=0.5,color='#283149')\n",
    "    plt.plot([i*4+0.5,i*4+1.5],[np.median(mse_list)]*2,'-',lw=0.9,color='#DA0463')\n",
    "plt.grid('--k',lw=0.5)\n",
    "plt.title(chunk,fontsize=15)\n",
    "plt.ylabel('MSE',fontsize=13)\n",
    "plt.xlabel('Number of linear combinations of substitutions',fontsize=13)\n",
    "plt.savefig('/nfs/scistore08/kondrgrp/eputints/Jupyter/HIS3InterspeciesEpistasis/Analysis/Katya/NN/complexity/20_iterations/mse_'+chunk+'.pdf')\n",
    "\n",
    "plt.figure(figsize=[6,7])\n",
    "for i,r2_list in enumerate(r2_weights):\n",
    "    plt.plot([i*4+1]*len(r2_list),r2_list,'o',alpha=0.5,color='#283149')\n",
    "    plt.plot([i*4+0.5,i*4+1.5],[np.median(r2_list)]*2,'-',lw=0.9,color='#DA0463')\n",
    "plt.ylim(-1,1)\n",
    "plt.title(chunk,fontsize=15)\n",
    "plt.grid('--k',lw=0.5)\n",
    "plt.ylabel('Median Spearman R between sets of weights',fontsize=13)\n",
    "plt.xlabel('Number of linear combinations of substitutions',fontsize=13)\n",
    "plt.savefig('/nfs/scistore08/kondrgrp/eputints/Jupyter/HIS3InterspeciesEpistasis/Analysis/Katya/NN/complexity/20_iterations/r2_weights_'+chunk+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAGfCAYAAACdqpz+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMBaECSvDMAzDMIwF8f8BZU8gb3SDVoIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aff24a1e0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitness_potential = x_train.dot(weights)+biases\n",
    "plt.figure(figsize = [10,7])\n",
    "plt.plot(fitness_potential[:10000], y_train[:10000], 'ok', alpha = 0.01);\n",
    "plt.plot(fitness_potential[:10000], predicted_train[:10000], '.b', alpha = 0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for i,l in enumerate(mse_val):\n",
    "\n",
    "        x.extend([i*3+1]*len(l))\n",
    "        y.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2b2f37ceb250>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnEAAAG5CAYAAADh3mJ8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRkupmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2f37c68590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10,7])\n",
    "sns.regplot(x=np.array(x), y=np.array(y))\n",
    "plt.grid('--k',lw=.5)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('# neurons first layer')\n",
    "plt.title('Pearson coefficient is %.2f' % pearsonr(x,y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
