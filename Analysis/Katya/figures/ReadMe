1.1		1 layer: 1 neuron -- ReLU activation function. Adargad optimizer. Random, normally distributed initial weights.
1.2		Loss function history (MSE)
1.3		Fitness potential vs. actual values.

2.1		1 layer: 1 neuron -- ReLU activation function. SGD optimizer. Random, normally distributed initial weights.
2.2		Loss function history (MSE)

3.1		1 layer: 1 neuron -- Sigmoid activation function. SGD optimizer. Random, normally distributed initial weights.
3.2		Loss function history (MSE)
3.3		Fitness potential vs. actual values. In blue the predicted values are plotted (and, therefore, the model being fitted).
3.4		R2 values for actual vs predicted values
3.5		The resulting MSE

4.1		2 layers: 100 neurons -- ReLU, 1 neuron -- ReLU. Adargad optimizer. Random, normally distributed initial weights.

5.1		6 layers: 200 neurons -- ReLU ... (?) 1 neuron -- ReLU. Adargad optimizer. Random, normally distributed initial weights.

6.1		Negative control: shuffled fitness. 1 layer: 1 neuron -- ReLU activation function. Adargad optimizer. 
6.2		Loss function history (MSE)

7.1		3 layers: 1 neuron -- Sigmoid, 10 neurons -- Sigmoid, 1 neuron -- Sigmoid. Adagrad optimizer. Random, normally distributed initial weights.
7.2		Loss function history (MSE)
7.3		Fitness potential vs. actual values. In blue the predicted values are plotted (and, therefore, the model being fitted).
7.4		R2 values for actual vs predicted values
7.5		The resulting MSE
7.6		10-fold crossvalidation, R2 scores statistics
7.7		2-fold crossvalidation, R2 scores statistics

8.1		3 layers: 1 neuron -- TanH, 10 neurons -- TanH, 1 neuron -- TanH. Adagrad optimizer. Random, normally distributed initial weights.
8.2		Loss function history (MSE)
8.3		Fitness potential vs. actual values. In blue the predicted values are plotted (and, therefore, the model being fitted).
8.4		R2 values for actual vs predicted values
8.5		The resulting MSE

9.1		3 layers: 3 neurons -- Sigmoid, 10 neurons -- Sigmoid, 1 neuron -- Sigmoid. Adagrad optimizer. Random, normally distributed initial weights.
9.2		Loss function history (MSE)
9.3		Fitness potential (weights[:,0]) vs. actual values. In blue (weights[:,0]), pink (weights[:,1]) and orange (weights[:,2]) the predicted values are plotted (and, therefore, the model being fitted).
9.4		R2 values for actual vs predicted values
9.5		The resulting MSE
9.6		R2 values for weights in different neurons of the first layer

10.1		6 layers: 3 neurons -- Sigmoid, 100 neurons -- Sigmoid, 50 neurons -- Sigmoid, 10 neurons -- Sigmoid, 100 neurons -- Sigmoid, 1 neuron -- Sigmoid. Adagrad optimizer. Random, normally distributed initial weights.
10.2		Loss function history (MSE)
10.3		Fitness potential (weights[:,0]) vs. actual values. In blue (weights[:,0]), pink (weights[:,1]) and orange (weights[:,2]) the predicted values are plotted (and, therefore, the model being fitted).
10.4		R2 values for actual vs predicted values
10.5		The resulting MSE
10.6		R2 values for weights in different neurons of the first layer

11.1		3 layers: 100 neurons -- Sigmoid, 10 neurons -- Sigmoid, 1 neuron -- Sigmoid. Adagrad optimizer. Random, normally distributed initial weights.
11.2		Loss function history (MSE)
11.3		Fitness potential (weights[:,0]) vs. actual values. In blue (weights[:,0]), pink (weights[:,50]) and orange (weights[:,99]) the predicted values are plotted (and, therefore, the model being fitted).
11.4		R2 values for actual vs predicted values
11.5		The resulting MSE
11.6		R2 values for weights in different neurons of the first layer

12.1		3 layers: 1 neuron -- ReLU, 10 neurons -- ReLU, 1 neuron -- ReLU. Adagrad optimizer. Random, normally distributed initial weights. Fitness potential vs. actual values. In blue the predicted values are plotted (and, therefore, the model being fitted).




